{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source RAG - Leveraging Hugging Face Endpoints through LangChain\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "-  Breakout Room #2:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating LangChain components powered by the endpoints\n",
        "  4. Preparing Data!\n",
        "  5. Simple LCEL RAG Chain"
      ],
      "metadata": {
        "id": "lcW6UWldWUMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "-spIWt2J3Quk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-huggingface langchain-community faiss-cpu huggingface-hub==0.27.0"
      ],
      "metadata": {
        "id": "EwGLnp31jXJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8193201b-e271-4727-a6ac-6524fc3738bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/450.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m440.3/450.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ],
      "metadata": {
        "id": "SpZTBLwK3TIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ],
      "metadata": {
        "id": "NspG8I0XlFTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c260724a-5424-4928-9fcc-2a00a83924d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ],
      "metadata": {
        "id": "QMru14VBZAtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ],
      "metadata": {
        "id": "TGooehdzcmPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_LLM_ENDPOINT_URL = \"https://ujdfm13gtllngixi.us-east-1.aws.endpoints.huggingface.cloud\""
      ],
      "metadata": {
        "id": "N7u2Tu1FsURh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=f\"{YOUR_LLM_ENDPOINT_URL}\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Cz6Mrnt2ku",
        "outputId": "954ca0f8-733c-487e-d1f5-2d1ef86ea466"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ],
      "metadata": {
        "id": "fun4XrRxZK9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "OFAbFT91Z8QV",
        "outputId": "57e24b8c-cb25-4c4b-978d-5a03044cb519"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I am doing well, thanks for asking. I have been busy with work and other things, but I always make time for my blog. Today, I want to talk about something that I think is very important: self-care.\\nSelf-care is not just about taking a relaxing bath or getting a massage (although those things are nice too). It's about taking care of your physical, emotional, and mental health. It's about making time for activities that nourish your mind, body, and spirit.\\nAs someone who has struggled with anxiety and depression, I know how easy it is to neglect self-care. When you're feeling overwhelmed, it can be hard to remember to take care of yourself. But trust me, it's worth it. Self-care is not selfish; it's essential.\\nHere are some self-care tips that I want to share with you:\\n1. Get enough sleep: This one is a no-brainer. When you're well-rested, you're more productive, more focused, and more resilient. Aim for 7-8 hours of sleep each night.\\n2. Exercise regularly: Exercise releases endorphins, which are natural mood-boosters. Find an activity that you enjoy, whether it's walking, running, swimming, or dancing.\\n3. Eat a healthy diet: Fuel your body with whole, nutritious foods like fruits, vegetables, whole grains, and lean proteins. Avoid sugary and processed foods that can drain your energy.\\n4. Practice mindfulness: Mindfulness is the practice of being present in the moment, without judgment. You can practice mindfulness through meditation, deep breathing, or simply paying attention to your senses.\\n5. Connect with nature: Spending time in nature can be incredibly grounding and calming. Take a walk outside, go for a hike, or simply sit in a park or garden.\\n6. Prioritize relationships: Surround yourself with people who support and uplift you. Nurture those relationships by scheduling regular check-ins and activities.\\n7. Engage in activities that bring you joy: Whether it's reading, painting, or playing music, make time for activities that make you happy.\\n8. Learn to say no: Don't overcommit yourself by taking on too much. Learn to say no to things that don't align with your values or priorities.\\n9. Practice self-compassion: Be kind to yourself, especially when you make mistakes. Remember that everyone makes mistakes, and it's okay not to be perfect.\\n10. Seek help when you need it: If you're struggling with mental health\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can add a RAG-style prompt using Llama 3 Instruct's prompt templating!"
      ],
      "metadata": {
        "id": "ngH3fhw4aQ8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "<|start_header_id|>system<|end_header_id|>\n",
        "You are a helpful assistant. You answer user questions based on provided context. If you can't answer the question with the provided context, say you don't know.<|eot_id|>\n",
        "\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "User Query:\n",
        "{query}\n",
        "\n",
        "Context:\n",
        "{context}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "zdvv4JmkzEtj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple LCEL chain using our prompt template Runnable and our LLM Runnable."
      ],
      "metadata": {
        "id": "Oe0Qrzn4adzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = rag_prompt | hf_llm"
      ],
      "metadata": {
        "id": "CE4djpxM0-Fg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke({\"query\" : \"Who old is Carl?\", \"context\" : \"Carl is a sweet dude, he's 40.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PNwrLXqDxHDY",
        "outputId": "39ee374d-fa66-462f-e2fe-33f8f983e819"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Carl is 40 years old.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ],
      "metadata": {
        "id": "emGw4-66aBfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "YOUR_EMBED_MODEL_URL = \"https://fkjcnq8xeyywb1v4.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=YOUR_EMBED_MODEL_URL,\n",
        "    task=\"feature-extraction\",\n",
        ")"
      ],
      "metadata": {
        "id": "n9Q7e4Gnwe_C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a simple cosine-similarity function to verify our endpoint is working as expected."
      ],
      "metadata": {
        "id": "YXYRBqbBayWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(phrase_1, phrase_2):\n",
        "  vec_1 = hf_embeddings.embed_query(phrase_1)\n",
        "  vec2_2 = hf_embeddings.embed_query(phrase_2)\n",
        "  return np.dot(vec_1, vec2_2) / (norm(vec_1) * norm(vec2_2))"
      ],
      "metadata": {
        "id": "lOP6LKr74RG8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a few examples below!"
      ],
      "metadata": {
        "id": "uGZNhxF2bVIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(\"I love my fluffy dog!\", \"I adore this furry puppy!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o_cqEZ34f15",
        "outputId": "caa60677-2c35-4a3e-f816-7fe19bfd6c51"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8903063446222079"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(\"I love my fluffy dog!\", \"Trekking across the arctic is tough work.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1nsAV1n4w4a",
        "outputId": "c4163083-21a5-41fa-809d-c98b6af9a4bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6667445107282148"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Preparing Data!\n",
        "\n",
        "We'll start by loading some data from GitHub (Paul Graham's Essays) and then move to chunking them into manageable pieces!\n",
        "\n",
        "First - let's grab the repository where the files live."
      ],
      "metadata": {
        "id": "iiz6vKMlbbP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dbredvick/paul-graham-to-kindle.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkuzZben5Eqp",
        "outputId": "a6eaf592-d993-47db-d4f3-d44cbcc6e951"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'paul-graham-to-kindle'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 36 (delta 3), reused 31 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (36/36), 2.35 MiB | 19.06 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next - we can load them using LangChain!"
      ],
      "metadata": {
        "id": "8prMk6R0bsYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "document_loader = TextLoader(\"./paul-graham-to-kindle/paul_graham_essays.txt\")\n",
        "documents = document_loader.load()"
      ],
      "metadata": {
        "id": "K155zM7e53lt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's split them into 1000 character pieces."
      ],
      "metadata": {
        "id": "5wYfo6_0bwVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "len(split_documents)"
      ],
      "metadata": {
        "id": "w-Gx_0iL6Ikc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813fd597-b7ab-4325-c8e0-25f9a84acd8a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4265"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ],
      "metadata": {
        "id": "d5HrkDhTb4i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "for i in range(0, len(split_documents), 32):\n",
        "  if i == 0:\n",
        "    vectorstore = FAISS.from_documents(split_documents[i:i+32], hf_embeddings)\n",
        "    continue\n",
        "  vectorstore.add_documents(split_documents[i:i+32])"
      ],
      "metadata": {
        "id": "ucghQgRp6YXr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up FAISS as a retriever."
      ],
      "metadata": {
        "id": "q07ZUp6Db_AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "fXr-yrAq7h8V"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Simple LCEL RAG Chain\n",
        "\n",
        "Now we can set up our LCEL RAG chain!\n",
        "\n",
        "> NOTE: We're not returning context for this example, and only returning the text output from the LLM."
      ],
      "metadata": {
        "id": "sYrW6FRecO7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "lcel_rag_chain = {\"context\": itemgetter(\"query\") | hf_retriever, \"query\": itemgetter(\"query\")}| rag_prompt | hf_llm"
      ],
      "metadata": {
        "id": "ffIzIlct8ISb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lcel_rag_chain.invoke({\"query\" : \"What is the best part of Silicon Valley?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "HOQfkEgb8nPH",
        "outputId": "fdca107a-18a0-4291-e2ba-756f8f540867"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The best part of Silicon Valley, according to Paul Graham, is its proximity to a top-notch university, such as Stanford, which acts as a magnet to attract the best people from around the world. However, he also mentions that the area has room for improvement, particularly in terms of public transportation and urban planning, suggesting that a more pedestrian-friendly and bikeable town would be desirable.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        "Once you've completed this notebook, please move to the Chainlit portion of the assignment, located in the Week 8 Day 2 `README.md`."
      ],
      "metadata": {
        "id": "IDhfZf2DeKVo"
      }
    }
  ]
}